\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage[titletoc,title]{appendix}
\usepackage{pgfplots}

% \usepackage[T1]{fontenc}
% \usepackage{lmodern}
% \usepackage{amsmath,amsfonts,amssymb,mathtools}
\usepackage{graphicx,float}
\pgfplotsset{compat=1.18}
% \usepackage{minted}
% \usepackage{circuitikz}
% \usemintedstyle{trac}
\graphicspath{plots}


\usepackage[backend=biber,
    style=numeric-comp,
    maxcitenames=1,
    maxbibnames=1,
    %backref=true
    ]{biblatex}
\addbibresource{references.bib}
% \addbibresource{references_2.bib}

\title{My text title}
\author{Piotr Drabik, Mateusz Kojro}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
In this paper we present the feasibility study of using deep learning techniques to cryptographically attack components of the AES encryption algorithm. We compare the relative performance of spiking and traditional neural networks in regression tasks at different levels of complexity. We show that due to encoding issues (conversion from numeric values to spiking time series) SNN cannot be trivially applied to this kinds of problems.
\end{abstract}


\section{Introduction}
Advanced Encryption standard (AES) is one of the most widely used encryption algorithms, it's resilience to direct attacks guarantees safe data encryption. Simple construction of cipher guarantees fast encryption that can be chained together, further complicating any efforts of direct attack, unfortunately the simplicity of algorithm in the future may lead to mathematical solution, that reverses the algorithm without need for knowledge of used key.

Spiking neural networks (SNN) provide new deep-learning solution more closely resembling neurons found in nature. New neuron model might enable faster learning rate crucial when working with big sets of data.

\section{Motivation}

Deep learning has proven over the years its ability for modeling and solving problems that were thought to be nearly impossible. Given this track record and emergence of new ideas in this space problems once though to be exceptionally hard need to be examine more closely with the context of recent advancements. In this paper we will investigate the feasibility of using deep learning (namely standard neural networks and spiking neural networks) in attacks on the Rijmen's mix columns algorithm - a major component of the ubiquitous AES encryption method.

\section{Theoretical background}

\subsection{Advanced Encryption Standard}

Introduced by Joan Daemen and Vincent Rijmen in year 1998, encryption standard is a widely used, fixed block size, symmetric encryption algorithm. Adopted by U.S. AES superseded DES(Data Encryption Standard) in year 2002 \cite{Cryptanalysis of Block Ciphers}.

\subsubsection{Known Attacks}
To this day AES cipher has not been broken, meaning there are no known algorithms that can decipher an encrypted message without knowledge of used key, in time shorted than needed to perform brute-force attack.
In year 2002 a theoretical attack, named the "XSL attack", was announced by Nicolas Courtois and Josef Pieprzyk, the soul existence of mathematical solutions to the algorithm behind AES, poses questions about it's security. Provided by Courtois and Pieprzyk mathematical equations used to break the algorithm require enormous amounts of computational power and time, this problem might be mitigated by usage of neural-networks \cite{Cryptanalysis of Block Ciphers}.

\subsubsection{Building blocks}
AES consists of 4 main building blocks, static sized key and Information block. 

\begin{itemize}
\item Encryption Key, based on chosen block size key is sized accordingly. The size of used Key specifies number of "rounds" of encoding, for 128-bit keys it is 10 rounds of encoding, 12 for 192-bit keys, 14 for 256-bit. 
\item Information block, encoded data is spited into chunks of sizes: 128, 192 or 256 bit, those are encoded separately.  
\end{itemize}
Building blocks of AES algorithm:
\begin{itemize}
\item Byte Sub
S-Box(substitution-box) stage, used to obscure relationship between key and cipher text, thus ensuring Shannon's property of confusion.
\item Shift Row, a transposition step where the rows of the state are shifted cyclically a certain number of steps.
\item Mix Column, a linear mixing operation which operates on the columns of the state, combining the four bytes in each column.
\item Add Round Key, each byte of data block is combined with key with xor operation 
\end{itemize}
 To every chunk of message those stages are applied in succession, this operation is repeated multiple times depending on key size

\subsection{Spiking neural networks}

Despite its successes in many areas across number of industries broad field of deep learning is constantly expanding and evolving. One of the results of this process is the introduction of neuron models more closely resembling those found in the human brain. One of the most popular examples of this trend are "leaky integrate and fire" neurons. They allow for an introduction of time dependent activation function and doing so are overstepping one of the big simplifications introduced by traditional deep learning models. Networks composed of this kinds of neurons are often called spiking neural networks (SNN).

\section{Methods}
\subsection{Models}

In order to compare performance of SNN and NN we prepared two simple models:

\subsubsection{SNN Model}
\subsubsection{NN Model}

\subsection{Dataset}

In order to test and compare the introduced models <we> prepared 2 datasets consisting of randomly generated <...>

\begin{itemize}
    \item The training dataset: N encoded massages with the <some> message size
    \item Evaluation dataset composed of N messages with the same sizes as the training dataset
\end{itemize}

\section{Training}

We trained both models using the same dataset for <x> epochs with <y> minibatches and evaluated the performance using a testing dataset 


\section{Results}



\section{Discussion}

As we have shown in spite of the many advantages they offer SNN are not really well fit for regression type tasks like approximating mathematical functions. Main reason for that is the difficulty of encoding and decoding the spikes because there is no well known and understood method for this operation. Research done by \cite{kahana_function_2022} shows that there is a possibility of using auto encoders like DeepONet \cite{lu_learning_2021} to achieve this but the work is still in the early stages (\cite{kahana_function_2022} shows SNN with DeepONet assistance learning functions like $x^2$ and $\sin{x}$). Additionally while using SNN without specialized hardware\cite{bouvier_spiking_2019} the training time increases dramatically \cite{kahana_function_2022}.





\begin{figure}
    \input{./plots/xsqr_snn_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/xsqr_nn_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/xsqr_both_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/xsqr_binomial.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
    \centering
    \input{./plots/xsqr_binomial.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}


\begin{figure}
\input{./plots/2x_binomial.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/2x_both_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/2x_snn_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/2x_nn_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/2x_both_loss_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/2x_binomial.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}


\begin{figure}
\input{./plots/loss_over_time_for_Rijndael_MixColumns_function_SNN.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/loss_over_time_for_Rijndael_MixColumns_function_NN.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/loss_over_time_for_Rijndael_MixColumns_function_SNN_and_NN.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/loss_over_time_for_Rijndael_MixColumns_function_SNN_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/loss_over_time_for_Rijndael_MixColumns_function_NN_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/loss_over_time_for_Rijndael_MixColumns_function_SNN_and_NN_histogram.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\begin{figure}
\input{./plots/Rijndael_MixColumns_function_binomial.pgf}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\printbiliography

\end{document}
